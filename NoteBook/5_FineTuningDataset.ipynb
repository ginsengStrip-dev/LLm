{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebe289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted_recipes.json has been created in the desired Person 1 / Person 2 format!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Read your dataset file\n",
    "with open(\"../Data/dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split recipes using regex for lines starting with number + dot\n",
    "recipes = re.split(r\"\\n\\d+\\.\\s\", data)\n",
    "recipes = [r.strip() for r in recipes if r.strip()]  # Remove empty entries\n",
    "\n",
    "formatted_recipes = []\n",
    "\n",
    "for recipe in recipes:\n",
    "    lines = recipe.split(\"\\n\", 1)  # Split into title and rest\n",
    "    if len(lines) == 2:\n",
    "        title = lines[0].strip()\n",
    "        instructions = lines[1].strip()\n",
    "        \n",
    "        formatted_recipes.append(f\"<|startoftext|>Person 1<|separator|>{title}<|endoftext|>\")\n",
    "        formatted_recipes.append(f\"<|startoftext|>Person 2<|separator|>{instructions}<|endoftext|>\")\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"formatted_recipes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(formatted_recipes, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"formatted_recipes.json has been created in the desired Person 1 / Person 2 format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f322bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed messages: 5132\n",
      "Grouped messages: 2381\n",
      "Saved to: ../Data/parsed_grouped.json\n"
     ]
    }
   ],
   "source": [
    "# import re, json, os\n",
    "\n",
    "# file_path = \"../Data/dataset.txt\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# # --- Normalize NBSP / Unicode spaces ---\n",
    "# def normalize_line(line: str) -> str:\n",
    "#     return line.replace(\"\\u202f\", \" \").replace(\"\\u00A0\", \" \").replace(\"\\u2009\", \" \").rstrip(\"\\n\").strip()\n",
    "\n",
    "# lines = [normalize_line(l) for l in lines]\n",
    "\n",
    "# # --- Filters for unwanted system/media lines ---\n",
    "# encryption_message = \"Messages and calls are end-to-end encrypted.\"\n",
    "# media_pattern = \"<Media omitted>\"\n",
    "# edited_message = \"<This message was edited>\"\n",
    "# deleted_message = \"You deleted this message\"\n",
    "# created_group_message = \"created group\"\n",
    "# added_you_to_group_message = \"added you\"\n",
    "# null_message = \"null\"\n",
    "# tagging_pattern = r'@[\\w]+'\n",
    "# email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}'\n",
    "# url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# # --- WhatsApp-style timestamp regex ---\n",
    "# pattern = re.compile(\n",
    "#     r'^\\s*\\[?(\\d{1,2}/\\d{1,2}/\\d{2,4}),\\s*'                      # date\n",
    "#     r'(\\d{1,2}:\\d{2}(?:\\s*(?:AM|PM|am|pm))?)\\]?\\s*'               # time (optional AM/PM)\n",
    "#     r'[-‚Äì‚Äî]\\s*'                                                  # dash\n",
    "#     r'(.+?):\\s*'                                                 # sender\n",
    "#     r'(.*)$',                                                    # message\n",
    "#     flags=re.UNICODE\n",
    "# )\n",
    "\n",
    "# parsed = []\n",
    "# for raw_line in lines:\n",
    "#     line = raw_line.strip()\n",
    "#     if not line:\n",
    "#         continue\n",
    "\n",
    "#     # Skip system lines\n",
    "#     if (encryption_message in line or deleted_message in line or media_pattern in line or\n",
    "#         created_group_message in line or added_you_to_group_message in line or\n",
    "#         line.strip().lower() == null_message or re.search(email_pattern, line) or re.search(url_pattern, line)):\n",
    "#         continue\n",
    "\n",
    "#     line = line.replace(edited_message, \"\").strip()\n",
    "#     line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "\n",
    "#     m = pattern.match(line)\n",
    "#     if m:\n",
    "#         date, time, sender, message = m.groups()\n",
    "#         parsed.append({\"timestamp\": f\"{date}, {time}\", \"sender\": sender.strip(), \"message\": message.strip()})\n",
    "#     else:\n",
    "#         # continuation line ‚Üí append to last message\n",
    "#         if parsed:\n",
    "#             parsed[-1][\"message\"] += \"\\n\" + line\n",
    "#         else:\n",
    "#             parsed.append({\"timestamp\": \"\", \"sender\": \"UNKNOWN\", \"message\": line})\n",
    "\n",
    "# print(f\"Parsed messages: {len(parsed)}\")\n",
    "\n",
    "# # --- Group consecutive messages by sender ---\n",
    "# grouped_messages = []\n",
    "# for entry in parsed:\n",
    "#     sender, message = entry[\"sender\"], entry[\"message\"]\n",
    "#     if grouped_messages and grouped_messages[-1][\"sender\"] == sender:\n",
    "#         grouped_messages[-1][\"message\"] += \"\\n\" + message\n",
    "#     else:\n",
    "#         grouped_messages.append({\"sender\": sender, \"message\": message})\n",
    "\n",
    "# print(f\"Grouped messages: {len(grouped_messages)}\")\n",
    "\n",
    "# # --- Example: build fine-tuning sequences ---\n",
    "# start_tok, sep_tok, end_tok = \"<|startoftext|>\", \"<|separator|>\", \"<|endoftext|>\"\n",
    "# fine_tuning_sequences = [f\"{start_tok}{g['sender']}{sep_tok}{g['message']}{end_tok}\" for g in grouped_messages]\n",
    "\n",
    "# out_path = \"../Data/parsed_grouped.json\"\n",
    "# with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(fine_tuning_sequences, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"Saved to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d171e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sequence 1 ---\n",
      "<|startoftext|>Person 2<|separator|>Sagoo and milk\n",
      "Aru aku nai<|endoftext|>\n",
      "\n",
      "--- Sequence 2 ---\n",
      "<|startoftext|>Person 1<|separator|>ü´†\n",
      "Ooo\n",
      "Dia\n",
      "Pet tu bohruwa rh<|endoftext|>\n",
      "\n",
      "--- Sequence 3 ---\n",
      "<|startoftext|>Person 2<|separator|>Mane<|endoftext|>\n",
      "\n",
      "--- Sequence 4 ---\n",
      "<|startoftext|>Person 1<|separator|>Pet tu khali hbo ndiba\n",
      "In short<|endoftext|>\n",
      "\n",
      "--- Sequence 5 ---\n",
      "<|startoftext|>Person 2<|separator|>Ohhh\n",
      "Kham olop pisot<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(fine_tuning_sequences[:5], 1):\n",
    "    print(f\"\\n--- Sequence {i} ---\")\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines removed: 199\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "# encryption_message = \"Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\"\n",
    "# media_pattern = \"<Media omitted>\"\n",
    "# email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "# url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "# edited_message = \"<This message was edited>\"\n",
    "# deleted_message = \"You deleted this message\"\n",
    "# null_message = \"null\"\n",
    "# created_group_message = \"created group\"\n",
    "# added_you_to_group_message = \"added you\"\n",
    "# tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "\n",
    "# filtered_lines = []\n",
    "# for line in lines:\n",
    "#     if (\n",
    "#             encryption_message not in line and\n",
    "#             deleted_message not in line and\n",
    "#             null_message != line.split(\" \")[-1] and\n",
    "#             media_pattern not in line and\n",
    "#             created_group_message not in line and\n",
    "#             added_you_to_group_message not in line and\n",
    "#             not re.search(email_pattern, line) and\n",
    "#             not re.search(url_pattern, line)\n",
    "#     ):\n",
    "#         line = line.replace(edited_message, \"\").strip()\n",
    "#         line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "#         filtered_lines.append(line)\n",
    "\n",
    "# pattern = r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)'\n",
    "# content = '\\n'.join(filtered_lines)\n",
    "# messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "# lines_removed = len(lines) - len(filtered_lines)\n",
    "# print(f\"Lines removed: {lines_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6f178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouped_messages = []\n",
    "\n",
    "# for _, sender, message in messages:\n",
    "#     if grouped_messages and grouped_messages[-1][\"sender\"] == sender:\n",
    "#         grouped_messages[-1][\"message\"] += \"\\n\" + message\n",
    "#     else:\n",
    "#         grouped_messages.append({\n",
    "#             \"sender\": sender,\n",
    "#             \"message\": message\n",
    "#         })\n",
    "\n",
    "# len(grouped_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45793160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define special tokens\n",
    "# start_of_text_token = \"<|startoftext|>\"\n",
    "# end_of_text_token = \"<|endoftext|>\"\n",
    "# separator_token = \"<|separator|>\"\n",
    "\n",
    "# fine_tuning_data = []\n",
    "\n",
    "# for message in grouped_messages:\n",
    "#     sender = message[\"sender\"]\n",
    "#     message_text = message[\"message\"]\n",
    "#     input_sequence = f\"{start_of_text_token}{sender}{separator_token}{message_text}{end_of_text_token}\"\n",
    "#     fine_tuning_data.append(input_sequence)\n",
    "\n",
    "# len(fine_tuning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee0d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfine_tuning_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# fine_tuning_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# save_path = \"../Data/fine_tuning.json\"\n",
    "# with open(save_path, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(fine_tuning_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f8d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, json\n",
    "\n",
    "# # --- Example conversation pasted as string ---\n",
    "# raw_text = \"\"\"\n",
    "# Person1: Ar√©, ki khobor?\n",
    "# Person2: Moi bhal, toi kene ase?\n",
    "# Person1: Moi bhal, kintu office tu hectic hoi gol.\n",
    "# Person2: Haan yaar, amar life tu full busy.\n",
    "# Person1: Etiya bisi kaam, aru personal time nohoi.\n",
    "# Person2: Thik kotha. Kalir dinat relax koribo pare nai.\n",
    "# Person1: By the way, kal toi market loi gol√©?\n",
    "# Person2: Haan, thoda shopping kori asil.\n",
    "# Person1: Ki kinili?\n",
    "# Person2: Ekta shirt aru ek pair shoes.\n",
    "# Person1: Wah, nice! Moi tu ekhon mobile kiniboloi bhabisu.\n",
    "# Person2: Which one? Android ne iPhone?\n",
    "# Person1: Thinking about iPhone, but budget thora tight.\n",
    "# Person2: Haha, iPhone tu costly. Android best option hoi jabo.\n",
    "# Person1: True, Android tu user-friendly aru affordable.\n",
    "# Person2: Moi Samsung use kori, bhal experience ase.\n",
    "# Person1: Good. Moi maybe OnePlus check korim.\n",
    "# Person2: OnePlus ekdom fast phone. Value for money.\n",
    "# Person1: Aro toi recent movies dekhi ne?\n",
    "# Person2: Haan, kal ‚ÄúRaghav‚Äù buli ekta movie dekhi.\n",
    "# Person1: Kene lagil?\n",
    "# Person2: Dekhiboloi joruri nai. Story thik thak.\n",
    "# Person1: Haha, then skip kori dim.\n",
    "# Person2: Tu ki series dekhi ase eta?\n",
    "# Person1: Haan, Netflixot ekhon series start kori su.\n",
    "# Person2: Ketiya tu binge kori ase.\n",
    "# Person1: Last night 3 episodes continuous dekhi su.\n",
    "# Person2: Oof, nidra tu gusi gol niki?\n",
    "# Person1: Haha, nidra sacrifice kori series binge.\n",
    "# Person2: Typical! Moi tu thoda self-control ase.\n",
    "# Person1: Lucky! Moi tu series addicted.\n",
    "# Person2: By the way, toi exam preparation kene chalise?\n",
    "# Person1: Exam tu Octoberot, aru bisi syllabus baki ase.\n",
    "# Person2: Careful hoi, daily routine follow kor.\n",
    "# Person1: Moi try kori ase, but distractions bisi.\n",
    "# Person2: Phone tu silent kor, tetiya bhal hobo.\n",
    "# Person1: True, phone tu main problem.\n",
    "# Person2: Moi jetiya study koru, phone switch off kori diu.\n",
    "# Person1: Good idea. Moi next time try korim.\n",
    "# Person2: Eta short-term sacrifice, long-term success.\n",
    "# Person1: Motivational speech diya jabo toi.\n",
    "# Person2: Haha, motivational guru banibo pare.\n",
    "# Person1: Ekdom true.\n",
    "# Person2: Chal, ajir dinner plan ase?\n",
    "# Person1: Nah, gharot simple kham.\n",
    "# Person2: Moi tu biryani order korim buli bhabisu.\n",
    "# Person1: Wah! Moi tu biryani bhal pau.\n",
    "# Person2: Next time together order korim.\n",
    "# Person1: Sure, deal!\n",
    "# Person2: Ok, aji logot call koru aru discuss koru.\n",
    "# Person1: Done, 9pm call korim.\n",
    "# \"\"\".strip()\n",
    "\n",
    "# # --- Step 1: Parse lines into (sender, message) ---\n",
    "# messages = []\n",
    "# for line in raw_text.splitlines():\n",
    "#     if \":\" in line:\n",
    "#         sender, msg = line.split(\":\", 1)\n",
    "#         messages.append((sender.strip(), msg.strip()))\n",
    "\n",
    "# # --- Step 2: Group consecutive messages from same sender ---\n",
    "# grouped_messages = []\n",
    "# for sender, message in messages:\n",
    "#     if grouped_messages and grouped_messages[-1][\"sender\"] == sender:\n",
    "#         grouped_messages[-1][\"message\"] += \"\\n\" + message\n",
    "#     else:\n",
    "#         grouped_messages.append({\"sender\": sender, \"message\": message})\n",
    "\n",
    "# # --- Step 3: Add tokens for fine-tuning ---\n",
    "# start_of_text_token = \"<|startoftext|>\"\n",
    "# end_of_text_token = \"<|endoftext|>\"\n",
    "# separator_token = \"<|separator|>\"\n",
    "\n",
    "# fine_tuning_data = []\n",
    "# for gm in grouped_messages:\n",
    "#     seq = f\"{start_of_text_token}{gm['sender']}{separator_token}{gm['message']}{end_of_text_token}\"\n",
    "#     fine_tuning_data.append(seq)\n",
    "\n",
    "# # --- Step 4: Save JSON ---\n",
    "# save_path = \"fine_tuning.json\"\n",
    "# with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(fine_tuning_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(f\"Saved {len(fine_tuning_data)} sequences to {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
